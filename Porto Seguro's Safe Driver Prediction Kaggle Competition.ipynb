{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Nov 12 17:44:09 2017\n",
    "\n",
    "@author: Kathan Sheth & Preyas Shah\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from feature import *\n",
    "import numpy as np\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Script by https://www.kaggle.com/ogrellier\n",
    "# Code: https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None, \n",
    "                  tst_series=None, \n",
    "                  target=None, \n",
    "                  min_samples_leaf=1, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior  \n",
    "    \"\"\" \n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean \n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index \n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)\n",
    "\n",
    "trainingDataCSV = \"Dataset/train.csv\"\n",
    "testDataCSV = \"Dataset/test.csv\"\n",
    "train = pd.read_csv(trainingDataCSV)\n",
    "test = pd.read_csv(testDataCSV)\n",
    "#Undersampling \n",
    "desired_apriori=0.10\n",
    "\n",
    "# Get the indices per target value\n",
    "idx_0 = train[train.target == 0].index\n",
    "idx_1 = train[train.target == 1].index\n",
    "\n",
    "# Get original number of records per target value\n",
    "nb_0 = len(train.loc[idx_0])\n",
    "nb_1 = len(train.loc[idx_1])\n",
    "\n",
    "# Calculate the undersampling rate and resulting number of records with target=0\n",
    "undersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\n",
    "undersampled_nb_0 = int(undersampling_rate*nb_0)\n",
    "\n",
    "# Randomly select records with target=0 to get at the desired a priori\n",
    "undersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n",
    "\n",
    "# Construct list with remaining indices\n",
    "idx_list = list(undersampled_idx) + list(idx_1)\n",
    "\n",
    "# Return undersample data frame\n",
    "train = train.loc[idx_list].reset_index(drop=True)\n",
    "\n",
    "train = train.replace(-1,np.nan)\n",
    "train.pop('id')\n",
    "test = test.replace(-1,np.nan)\n",
    "ids = test.pop('id')\n",
    "\n",
    "train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], \n",
    "                             test[\"ps_car_11_cat\"], \n",
    "                             target=train.target, \n",
    "                             min_samples_leaf=100,\n",
    "                             smoothing=10,\n",
    "                             noise_level=0.01)\n",
    "\n",
    "y = train.pop('target')\n",
    "X = train\n",
    "nanThresh = int(X.shape[0]*0.75) #75 % of non-null values should be there\n",
    "droppedFeatures = X.isnull().sum() > 0.25*X.shape[0]\n",
    "X.dropna(axis=1,thresh=nanThresh,inplace=True)\n",
    "#Fill the NaN Values\n",
    "nanValDict = {}\n",
    "modeFrame = X.mode(axis=0)\n",
    "maxArray = X.max(axis=0)\n",
    "for columnName in X.columns.values:\n",
    "    if 'cat' in columnName:\n",
    "        nanValDict[columnName] = int(maxArray[columnName] + 1)\n",
    "    elif 'bin' in columnName:\n",
    "        nanValDict[columnName] = int(modeFrame[columnName].values[0])\n",
    "    elif X[columnName].dtype == int:\n",
    "        nanValDict[columnName] = int(modeFrame[columnName].values[0])\n",
    "    else:\n",
    "        nanValDict[columnName] = X[columnName].mean()\n",
    "X = X.fillna(value=nanValDict)\n",
    "\n",
    "# for columnName in X.columns.values:\n",
    "#     if 'cat' in columnName or 'bin' in columnName:\n",
    "#         continue\n",
    "#     else:\n",
    "#         gpa_mean,gpa_std = X[columnName].mean(),X[columnName].std()\n",
    "#         X.loc[:,columnName] = (X[columnName] - gpa_mean) / gpa_std\n",
    "    #print (columnName)\n",
    "df_new = X\n",
    "for columnName in X.columns.values:    \n",
    "    if 'cat' not in columnName or columnName == 'ps_car_11_cat':\n",
    "        continue\n",
    "    else:\n",
    "        dummies = pd.get_dummies(X[columnName], prefix = columnName)\n",
    "        df_new = pd.concat([df_new, dummies], axis=1)\n",
    "        df_new.drop(columnName,axis=1,inplace=True)\n",
    "X = df_new\n",
    "\n",
    "#Handle the Special Categorial Feature\n",
    "    \n",
    "X['ps_car_11_cat_te'] = train_encoded\n",
    "X.drop('ps_car_11_cat', axis=1, inplace=True)\n",
    "\n",
    "# for columnName in X.columns.values:\n",
    "#      print (columnName)\n",
    "#X_resampled, y_resampled = SMOTE().fit_sample(X.values, y.values)\n",
    "clf = RandomForestClassifier(n_estimators=50,min_samples_leaf=2,min_samples_split=7,max_features=8)\n",
    "clf.fit(X,y)\n",
    "uselessFeatures = []\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(clf.feature_importances_)[::-1]\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print (\"%2d) %-*s %f\" % (f + 1, 30,X.columns.values[indices[f]], importances[indices[f]]))\n",
    "for i in range(len(importances)):\n",
    "    if(importances[i] < 0.001):\n",
    "        uselessFeatures.append(X.columns.values[i])\n",
    "for columnName in X.columns.values:\n",
    "    if(columnName in uselessFeatures):\n",
    "        X.drop(columnName,axis=1,inplace=True)\n",
    "print (X.shape)\n",
    "# clf = RandomForestClassifier(n_estimators=50)\n",
    "# use a full grid over all parameters\n",
    "# specify parameters and distributions to sample from\n",
    "# param_dist = {\"max_depth\": [3, None],\n",
    "#               \"max_features\": sp_randint(1, 11),\n",
    "#               \"min_samples_split\": sp_randint(2, 11),\n",
    "#               \"min_samples_leaf\": sp_randint(1, 11),\n",
    "#               \"bootstrap\": [True, False]}\n",
    "\n",
    "# run randomized search\n",
    "# n_iter_search = 20\n",
    "# random_search = RandomizedSearchCV(clf, param_distributions=param_dist,scoring='f1_micro',\n",
    "#                                    n_iter=n_iter_search,return_train_score=False)\n",
    "# random_search.fit(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,stratify=y)\n",
    "#clf = RandomForestClassifier(n_estimators=50,min_samples_leaf=1,min_samples_split=4,max_features=6,bootstrap=False)\n",
    "#clf.fit(Xtrain,Ytrain)\n",
    "#Youtput = clf.predict(Xtest)\n",
    "#print precision_recall_fscore_support(Ytest, Youtput, average='micro')\n",
    "\n",
    "# Fit classifier with out-of-bag estimates\n",
    "#clf = GradientBoostingClassifier(n_estimators=1200, learning_rate=0.01)\n",
    "#max_depth= [2,3,4,5,6,7]\n",
    "#param_grid = dict(max_depth=max_depth)\n",
    "#kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "#grid_search = GridSearchCV(clf, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "#grid_result = grid_search.fit(X, y)\n",
    "#print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "#params = {'n_estimators': 1200, 'max_depth': 3, 'subsample': 0.5,\n",
    "#          'learning_rate': 0.001, 'min_samples_leaf': 2, 'random_state': 3}\n",
    "#clf = GradientBoostingClassifier(n_estimators= 1200, max_depth= 3, subsample= 0.4,\n",
    "#          learning_rate= 0.01, min_samples_leaf= 2, random_state= 3)\n",
    "\n",
    "#clf.fit(X_train, y_train)\n",
    "#acc = clf.score(X_test, y_test)\n",
    "#print (\"Accuracy: {:.4f}\".format(acc))\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(cache_size=200, class_weight={0:0.1,1:0.9}, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "clf.fit(X_train, y_train)\n",
    "acc = clf.score(X_test, y_test)\n",
    "print (\"Accuracy: {:.4f}\".format(acc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#####################Test####################################\n",
    "X = test\n",
    "# nanThresh = int(X.shape[0]*0.75) #75 % of non-null values should be there\n",
    "for columnName in X.columns.values:\n",
    "    if(droppedFeatures[columnName]):\n",
    "        X.drop(columnName,axis=1,inplace=True)\n",
    "#Fill the NaN Values\n",
    "nanValDict = {}\n",
    "modeFrame = X.mode(axis=0)\n",
    "maxArray = X.max(axis=0)\n",
    "for columnName in X.columns.values:\n",
    "    if 'cat' in columnName:\n",
    "        nanValDict[columnName] = int(maxArray[columnName] + 1)\n",
    "    elif 'bin' in columnName:\n",
    "        nanValDict[columnName] = int(modeFrame[columnName].values[0])\n",
    "    elif X[columnName].dtype == int:\n",
    "        nanValDict[columnName] = int(modeFrame[columnName].values[0])\n",
    "    else:\n",
    "        nanValDict[columnName] = X[columnName].mean()\n",
    "X = X.fillna(value=nanValDict)\n",
    "\n",
    "# for columnName in X.columns.values:\n",
    "#     if 'cat' in columnName or 'bin' in columnName:\n",
    "#         continue\n",
    "#     else:\n",
    "#         gpa_mean,gpa_std = X[columnName].mean(),X[columnName].std()\n",
    "#         X.loc[:,columnName] = (X[columnName] - gpa_mean) / gpa_std\n",
    "        \n",
    "df_new = X\n",
    "for columnName in X.columns.values:    \n",
    "    if 'cat' not in columnName or columnName == 'ps_car_11_cat':\n",
    "        continue\n",
    "    else:\n",
    "        dummies = pd.get_dummies(X[columnName], prefix = columnName)\n",
    "        df_new = pd.concat([df_new, dummies], axis=1)\n",
    "        df_new.drop(columnName,axis=1,inplace=True)\n",
    "X = df_new\n",
    "\n",
    "X['ps_car_11_cat_te'] = test_encoded\n",
    "X.drop('ps_car_11_cat', axis=1, inplace=True)\n",
    "\n",
    "for columnName in X.columns.values:\n",
    "    if(columnName in uselessFeatures):\n",
    "        X.drop(columnName,axis=1,inplace=True)\n",
    "Youtput = clf.predict_proba(X)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "Youtput = pd.DataFrame(data=Youtput,columns=['target1','target2'])\n",
    "final_frame = pd.concat([ids,Youtput],axis=1)\n",
    "final_frame.to_csv('final_output4.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
